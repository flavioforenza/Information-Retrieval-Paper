\section*{Experimental Results}

The dataset used for the experiments is the famous \emph{Recipes1M+} \cite{15}, a collection 
created by MIT, consisting of more than one million culinary recipes. Of 
all these recipes, only a subset of 51235 documents of it was used due to their 
informative content which best fits the purpose of this study. The information 
about the line distributions for each recipe indicates that the instruction 
field contains a higher number than the information contained in the ingredients 
field. To have good performance in finding, we preferred to choose the 
"instructions" field (Fig. \ref{distributions}). The first ranking of relevant documents, generated 
by the tf-idf method, was evaluated following two different approaches that 
could best be linked with the principle explained in \cite{16}. Specifically, each 
category of a recipe is considered as an entity associated with a document. 
Both approaches rely on being able to compare the categories of relevant 
documents with the category of the target document. Since the categories 
are not present in the dataset, the two proposed approaches deal with being 
able to "extract" these categories directly from the web. The first approach 
uses an API, called \href{https://pypi.org/project/scrape-schema-recipe/}{\emph{Scrape Schema Recipe}}, which takes the link associated 
with the recipe website, present in the dataset, and scrapes the corresponding 
category. Unfortunately not all the links are still existing, therefore the 
evaluation of the ranking takes place following three types of methods:
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.7 \linewidth]{images/displot.png}
    \centering
    \caption{Distributions of lines per ingredients and instructions.}
    \label{distributions}
\end{figure}
\begin{enumerate}
    \item {\bfseries Overestimated}: consider the uncategorized document as good;
    \item {\bfseries Underestimated}: considers the uncategorized document not good;
    \item {\bfseries Discarded}: Discard the uncategorized document from the evaluation.
\end{enumerate}
As for the second method, this makes use of the \href{https://fdc.nal.usda.gov/api-guide.html}{\emph{USDA}} API, a library that 
has the task of taking every single ingredient of the recipe and fetching the 
corresponding category from a large database belonging to the United States 
Department of Agriculture. Compared to the first approach, the recipes that 
did not have a category are only four recipes. It is worth mentioning that both 
approaches are computationally expensive as the extraction of all categories 
took about two days. In addition to all these evaluations, it was decided to 
create a last one derived from a mixed approach (Scrape + USDA). Taking 
five random queries, the evaluations of the corresponding rankings are those 
reported in table (\ref{avgp}). 
\begin{table}[htbp]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{|c||c|c|c||c||c||}
        \hline
        \multirow{2}{*}{\bfseries{Queries}} & \multicolumn{3}{c||}{\bfseries{Scrape Schema Recipe}} & \multicolumn{1}{c||}{\bfseries{USDA}} & \multicolumn{1}{c||}{\bfseries{Mixed (Scrape+USDA)}} \\            & \bfseries{Overestimate} & \bfseries{Underestimate} & \bfseries{Discarded} & \bfseries{}  & \bfseries{}\\
        \hline
        \hline
        \RN{1} & 0.7520 & 0.3734 & 0.5958 & 0.9817 & 0.9947\\
        \hline
        \RN{2} & 0.9309 & 0.6780 & 0.9054 & 1.0 & 1.0\\
        \hline 
        \RN{3} & 0.7458 & 0.2746 & 0.5411 & 0.9797 & 0.9982\\
        \hline
        \RN{4} & 0.8939 &  0.5320 & 0.8433 & 0.9612 & 0.9870\\
        \hline
        \RN{5} & 0.8335 & 0.5033 & 0.7544 & 0.9940 & 0.9940\\
        \hline
        \hline
        Average & 0.8312 & 0.4722 & 0.7280 & 0.9833 &  \bfseries 0.99478 \\
        \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Average Precision on each query for each method.}
    \label{avgp}
\end{table}
As you can see, the mixed approach is the one that, in 
terms of average precision, manages to achieve the best evaluation. The 
performances, in terms of precision, recall and interpolated recall, visible in 
(Fig. \ref{Performance mixed approach}), demonstrate that the mixed approach is the best. 
\begin{figure}[h!]
    \centering
    \includegraphics[width =\linewidth]{images/11 keras/11 DOCUMENTS WITH MIXED ENTITIES (SCRAPE+USDA).png}
    \centering
    \caption{Performance in terms of Precion, Recall and Interpolated Recall of mixed approach (Scrape+USDA)}
    \label{Performance mixed approach}
\end{figure}
Moving 
on to the evaluation of the new queries generated, a visual comparison, based 
on a PCA, was used between the ranking generated by the tf-idf method and 
each new query produced by the project core (Fig. \ref{PCA}). 
\begin{figure}[h!]
    \centering
    \includegraphics[width =\linewidth]{images/PCA paper/PCA all.png}
    \centering
    \caption{Distance within the PCA between the original query (top) and the expanded query (bottom).}
    \label{PCA}
\end{figure}
The goal is to find a query 
whose distance from the target document is less than all other distances 
produced by the remaining queries. As we can see, compared to the position 
of the original query, a query with a smaller distance has always been found.
For each new query generated, the perplexity is calculated as the skip-grams 
step varies, with the language model of the target document. After various 
tests, with a higher number of queries, it can be stated that the skip-gram 
step and the perplexity are two inversely proportional measures, that is, as 
the step \emph{s} increases, the perplexity \emph{p} decreases. The same behavior occurs 
with the $\lambda_1$ and $\lambda_2$ parameters present in the interpolated smoothing. When  
$\lambda_1$ is less than  $\lambda_2$, perplexity always tends to decrease (Fig. \ref{perplexity}). 
\begin{figure}[h!]
    \centering
    \includegraphics[width =0.8\linewidth]{images/perplexity.png}
    \centering
    \caption{Variation of the perplexity value based on the change of the step \emph{s} and of the $\lambda_1$ and $\lambda_2$ values of interpolated smoothing.}
    \label{perplexity}
\end{figure}
We have come 
to the point of asking: How can the whole system be evaluated? To answer 
this question, four different libraries able to tokenize documents and queries 
were used: {\bfseries{Spacy}}, {\bfseries{Gensim}}, {\bfseries{Nltk}} and {\bfseries{Keras}}. Each of these produces different 
results in terms of perplexity, number of queries generated, sparsity index 
(Tab. \ref{performance}) and rankings(Tab. \ref{Index}). As can be seen from the data, each method has a different 
benefit. What each method has in common is the positioning of the target 
document in the first position of the ranking.
\begin{table}[h!]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{|c||c|c||c|c||c|c||c|c||c|c||}
        \hline
        \multirow{2}{*}{\bfseries{Method}} & \multicolumn{2}{c||}{\bfseries{\RN{1}}} & \multicolumn{2}{c||}{\bfseries{\RN{2}}} & \multicolumn{2}{c||}{\bfseries{\RN{3}}} & \multicolumn{2}{c||}{\bfseries{\RN{4}}} & \multicolumn{2}{c||}{\bfseries{\RN{5}}}\\            & \bfseries{T} & \bfseries{P} & \bfseries{T} & \bfseries{P} & \bfseries{T} & \bfseries{P} & \bfseries{T} & \bfseries{P} & \bfseries{T} & \bfseries{P}\\
        \hline
        \hline
        Keras & \color{red}{228} & \color{green}{0} & \color{red}{623} & \color{green}{0} & \color{red}{82} & \color{green}{0} & \color{red}{126} & \color{green}{0} & \color{red}{51} & \color{green}{0}\\
        \hline
        Nltk & \color{red}{228} & \color{green}{0} & \color{red}{623} & \color{green}{0} & \color{red}{82} & \color{green}{0} & \color{red}{126} & \color{green}{0} & \color{red}{51} & \color{green}{0}\\
        \hline 
        Spacy & \color{red}{221} & \color{green}{0} & \color{red}{647} & \color{green}{0} & \color{red}{81} & \color{green}{0} & \color{red}{141} & \color{green}{0} & \color{red}{72} & \color{green}{0}\\
        \hline
        Gensim & \color{red}{240} &  \color{green}{0} & \color{red}{652} & \color{green}{37} & \color{red}{84} & \color{green}{0} & \color{red}{113} & \color{green}{0} & \color{red}{37} & \color{green}{0}\\
        \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Target document position in the ranking of relevant documents. (T: \emph{tf-idf}, P:\emph{Proposed})}
    \label{Index}
\end{table}
\begin{table}[h!]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \bfseries{Method} & \bfseries{\#Queries} & \bfseries{Avg. Perplexity} &\bfseries{Avg. Sparsity Reduction}\\
        \hline
        \hline
        Keras & 14.900 & \bfseries{15.03} & 0.721\\
        \hline
        Nltk & 14.900 & 16.01 & 0.717\\
        \hline
        Spacy & 19.010 & 21.25 & \bfseries{0.741}\\
        \hline
        Gensim & \bfseries{22.090} & 33.96 & 0.730\\
        \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Performance in terms of number of queries (\#Queries), Average perplexity and Average Sparsity reduction with the SVD method.}
    \label{performance}
\end{table}

